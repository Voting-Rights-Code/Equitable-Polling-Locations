
<center><img src="README_Image.jpg" width="350"> </center>

# Equitable-Polling-Locations

The software component of this project is a tool that chooses an optimal set of polling locations from a set of potential locations. Optionally, it also gives a "best case scenario" by searching among the centroids of census block groups, which don't correspond to buildings or street corners, but are suggestive of what an ideal distribution might look like.

Unlike other optimization tools out there, which minimize the mean distance traveled or the maximal distance traveled, this tool (which minimizes the Kolm-Pollak, or KP, distance) does a bit of both. For a detailed description of the methods used here, see
[Horton et al.](https://arxiv.org/abs/2401.15452).
For more on the Kolm-Pollak distance and why it is suitable for optimizing with equity in mind, see the following papers: [Sherrif, Macguire](https://onlinelibrary.wiley.com/doi/10.1111/risa.13562); [Logan et al.](https://www.sciencedirect.com/science/article/abs/pii/S0198971520303239); [Kolm, 1976a](https://www.sciencedirect.com/science/article/abs/pii/0022053176900375); [Kolm, 1976b](https://www.sciencedirect.com/science/article/abs/pii/0022053176900685).

The ```result_analysis``` folder is an illustrative example of the type of analysis that can be done with the data generated by this code. The analysis code is in R.


### Example
In the following table, the first three rows have the same mean, while the last three rows have the same maximal distance traveled. The KP minimizing optimization allows the user to set an *aversion to inequality (beta)* parameter that defines a tradeoff between mean and standard deviation of the distances traveled. For a large enough beta, the optimization will choose the last distribution. For a smaller beta, it will choose the second row.

| Distances traveled  | Mean minimizing | Max minimizing | KP minimizing|
| ----- | ------ | ----- | ------ |
|.25, .25, .25, .25, 4 | Yes | | |
| .5, .5, .5, .5, 3| Yes | Yes | Depending on beta |
| .25, .25, .5, 1, 3 | Yes | Yes | Depending on beta |
| .5, .5, .5, .75, 3 |  | Yes |  |

### How it works
Given a set of existing and candidate polling locations, output the most equitable (by Kolm-Pollak distance) set of polling locations. The outputs of this model can be used to measure inequity among different racial groups in terms of access to polls (measured solely in terms of distance) and investigate how changes in choices and number of polling locations would change these inequities.

The algorithm for this model is as follows:
1. Create a list of potential polling locations
    1. Start with a list of historical polling locations
    1. Add to this a list of  buildings where one could feasibly site future polling locations
    1. Combine this data with a list of "best case scenario" polling locations, modeled by census block *group* centroids
1. Compute the distance from the centroid of each census block (representing residences) to each potential polling location (building or best case scenario)
    1. We average over census blocks rather than individual houses for computational feasibility
1. Compute the Kolm-Pollak weight from each block group to each polling location
    1. KP_factor  = e^(- beta * alpha * distance)
        1. beta is a user defined parameter
        1. alpha is a data derived normalization factor (alpha = (\sum (block population * distance_to_closest_poll)) / (\sum (block_population * distance_to_closest_poll^2))
    1. The KP_factor plays the role of a weighted distance in a standard objective function.
        1. The exponential in the KP_factor penalizes inequality in distances traveled
        1. For instance a group of 5 people all having to travel 1 mile to a polling location would have a lower KP_factor than a situation where 4 people travel 1/2 a mile while the fifth travels 3, even though the average distance traveled in both cases is the same.
1. Choose whether to minimize the average distance or the inequity penalized score (y_EDE) in the model
    1. Set beta = 0 for average distance
        1. In this case, minimize the average distance traveled
    1. Set beta in [-2, 0) for the inequity penalized score (y_EDE). The lower the beta, the greater penalty to inequality
        1. In this case, minimize (\sum block population * KP_factor)/ county population
1. Minimize the above according to the following constraints:
    1. Can only have a user specified number of polling locations open
    1. A user defined bound on the number of new locations
        1. Some maximal percent allowed to be new
        1. Some minimal percent that must have been a polling location in the past
        1. This can be easily modified to accommodate other needs (for example, require existing locations to remain open)
    1. Each census block can only be matched to one polling location
    1. Each census block must be matched to a single open precinct
    1. A user defined overcrowding constraint
1. The model returns a list of matchings between census blocks and polling locations, along with the distance between the two, and a demographic breakdown of the population.
1. The model then uses this matching and demographic data to compute a new data derived scaling factor (alpha), which it then uses to compute the inequity penalized score (y_EDE) for the matched system.

A FEW THINGS TO NOTE:
1. Currently, this model is run on census data, which counts voting age population. We make no assumptions about eligibility to vote, either in terms of citizenship, local disqualification laws or voter registration status.
2. When this model reports racial demographics, it uses Census categories for race and ethnicity. Namely, Ethnicity (Hispanic / Non-Hispanic) is orthogonal to race in the census data. Therefore, one may be Hispanic and Asian at the same time.

# To install
1. Clone main branch of Equitable-Polling-Locations
    1. This repo uses lfs. This can be downloaded from [https://git-lfs.com/].
        1. Download the appropriate version from this website and follow the instructions included there.
        1. If those instructions don't work, (as may be the case on Linux or MacOS), run ```sudo ./install.sh``` after downloading the file, then follow the instructions above. See [here](https://stackoverflow.com/questions/58796472/git-lfs-is-not-a-git-command-on-macos).
1. Install conda if you do not have it already
    1. This program uses SCIP as an optimizer, which is easily installed using Conda, but not using pip. (SCIP installation will be completed below by installing 'environment.yml')
    1. If you do not have conda installed already, use the relevant instructions [here](https://conda.io/projects/conda/en/latest/user-guide/install/index.html)
1. Create and activate conda environment. (Note, on a Windows machine, this requires using Anaconda Prompt.)
    1. `$ conda env create -f environment.yml`
    1. `$ conda activate equitable-polls`


# To run
From command line:

First activate the environment if not done so already:

```conda activate equitable-polls```

* In the directory of the Equitable-Polling-Locations git repo:
    * python ./model_run_cli.py -c NUM -l LOG_DIR ./path/to/config/file.yaml
        * NUM = number of cores to use for simultaneous runs (recommend <=4 for most laptops)
        * LOG_DIR = Where to put log files. The directory must exist, or will not run
        * path to config file accepts wild cards to set of sequential runs
        * For extra logging include the flag -vv
        * **Examples**
            * See ```python ./model_run_cli.py -h```
            * To run all expanded configs, parallel processing 4 at a time, and write log files out to the logs
    directory:
    ```python ./model_run_cli.py -c4 -l logs ./Gwinnett_County_GA_configs/Gwinnett_config_expanded_*.yaml```
            * To run all full configs run one at a time, extra logging printed to the console, and write log files out to the logs directory:
                ```python ./model_run_cli.py -vv -l logs ./Gwinnett_County_GA_configs/Gwinnett_config_full_*.yaml```
            * To run only the full_11 and write log files out to the logs directory:
        ```python ./model_run_cli.py -l logs ./Gwinnett_County_GA_configs/Gwinnett_config_full_11.yaml```
        * NOTE: BEWARE OF CAPITALIZATION. Both ./Gwinnett_County_GA_configs/Gwinnett* and ./Gwinnett_Ga_configs/Gwinnett* (note capitalization) will run on Windows. However, due to string replacement work in other parts of the programs, the former is preferred.


# Database

The default Google Cloud project used by the Voting Rights Code Group is ```equitable-polling-locations```.  To see the existing data in the Google Cloud Console, [click here](https://console.cloud.google.com/bigquery?ws=!1m4!1m3!3m2!1sequitable-polling-locations!2sequitable_polling_locations_prod). Access may be requested by reaching out to Voting Rights Code team members.

The output from the optimization model runs can be found in the BigQuery dataset equitable_polling_locations_prod.

## Schema and Tables Relationships

 All data written to the equitable_polling_locations_prod is intended to be immutable and, as such, there are no overwrites or deletions from subsequent runs against the same dataset.  Instead any time new optimization model data output is written, first an entry in the ```model_runs``` table is created and that will link the config used to all output tables.



| Name                       | Type  | Purpose                                                                            |
|----------------------------|-------|------------------------------------------------------------------------------------|
| model_configs              | Table | The config settings used to generate the optimization model output                 |
| model_runs                 | Table | Any time a model run is executed from a config, a new entry in model_runs is created.|
| model_config_runs          | View  | A view that inner joins model_configs and model_runs while only including the most recent and successful model_runs, avoiding any outdated data or incomplete output.
| edes                       | Table | The ede results from the optimization model run |
| edes_extras                | View  | A view that inner joins model_config_runs and edes                                 |
| precinct_distances         | Table | The precinct distances from the optimization model run |
| precinct_distances_extras  | View  | A view that inner joins model_config_runs and precinct_distances                   |
| residence_distances        | Table | The residence distances from from the optimization model run |
| residence_distances_extras | View  | A view that inner joins model_config_runs and residence_distances_extras           |
| results                    | Table | The general results from the from the optimization model run |
| results_extras             | View  | A view that inner joins model_config_runs and residence_distances_extras           |



### model_configs and model_runs
* One-to-Many Relationship:
  * A single model_configs record can have many associated model_runs records.
  * Each model_runs record belongs to exactly one model_configs record.

### ModelRuns and Related Tables
* One-to-Many Relationships:
  *  A single model_runs record can have many associated records in the following tables:
    *  results
    *  edes
    *  precinct_distances
    *  residence_distances

### Example Queries

Find all configs for for the config_set `Chatham_County_GA_no_bg_school_configs`:
```sql
SELECT *
  FROM `equitable-polling-locations.equitable_polling_locations_prod.model_config_runs`
 WHERE config_set = 'Chatham_County_GA_no_bg_school_configs';
```

Find all ede values for the config_set `Chatham_County_GA_no_bg_school_configs`, and config_name `Chatham_config_no_school_20`:
```sql
SELECT *
  FROM `equitable-polling-locations.equitable_polling_locations_prod.edes_extras`
 WHERE config_set = 'Chatham_County_GA_no_bg_school_configs'
   AND config_name = 'Chatham_config_no_school_20';
```


## BigQuery Table Management

Tables for the Equitable-Polling-Locations project are managed using Python's [SQLAlchemy](https://www.sqlalchemy.org/) and the [Alemebic](https://alembic.sqlalchemy.org/en/latest/) migration tool. See the folder `.../models` and `.../alembic` in this repository.  For example, the definition of the model_configs and model_runs tables can be found in `.../models/model_config.py`.

### Setup a new database or upgrading an existing one with the latest schema

Setting up a new database to work against is useful for development and testing.

To setup a new database:
1. Create a new dataset using the the [Google BigQuery Cloud Console](https://console.cloud.google.com/bigquery).
2. Activate conda `$ conda activate equitable-polls` (see earlier instructions)
3. Use the alembic upgrade command `$ alembic upgrade head`
4. (When prompted enter the project and dataset that was created if DB_PROJECT and/or DB_dataset were not set in the environment.)


### Adding a columns to an existing table

Alembic will manage the changes needed for database updates.

**NOTE:** Development and code changes involving updates to the database should be tested in scratch datasets, and only applied to the `equitable_polling_locations_prod` dataset after a code review and merge has been accepted.

1. Open the desired SQLALchemy model in the `.../models` directory.
2. Add a new column(s) as appropriate
3. Activate conda `$ conda activate equitable-polls` (see earlier instructions)
4. When complete, run the following alembic command to create a new migration.  `$ alembic revision --autogenerate -m "A SMALL DESCRIPTION OF YOUR CHANGES HERE"`.  This will create a new migration file in `.../alembic/versions` named after your description.
5. Alembic will create lines in your new migration file that looks similar to `op.create_foreign_key(None, 'model_runs', 'model_configs', ['model_config_id'], ['id'])` and `op.drop_constraint(None, 'model_runs', type_='foreignkey')`, these will need to be removed to work with BigQuery.
6. Upgrade your scratch database `$ alembic upgrade head`
7. Commit your changes in `.../models` and the added files in `.../alembic/versions` to git as appropriate

[See alembic documentation](https://alembic.sqlalchemy.org/en/latest/tutorial.html) for more information on migration management and command line options, including how to downgrade the database.

### Adding a new tables

Alembic will manage the changes needed for adding new database tables.

**NOTE:** Development and code changes involving updates to the database should be tested in scratch datasets, and only applied to the `equitable_polling_locations_prod` dataset after a code review and merge has been accepted.

1. Open the desired SQLALchemy model in the `.../models` directory.
2. Create a new model file(s) as appropriate
3. Update `.../models/__init__.py` to include your new model file(s)
4. Activate conda `$ conda activate equitable-polls` (see earlier instructions)
5. When complete, run the following alembic command to create a new migration.  `$ alembic revision --autogenerate -m "A SMALL DESCRIPTION OF YOUR CHANGES HERE"`.  This will create a new migration file in `.../alembic/versions` named after your description.
6. Upgrade your scratch database `$ alembic upgrade head`
7. Commit your changes in `.../models` and the added files in `.../alembic/versions` to git as appropriate

[See alembic documentation](https://alembic.sqlalchemy.org/en/latest/tutorial.html) for more information on migration management and command line options, including how to downgrade the database.

## Database Access

You must have a google account with access to a Google cloud project.

### Granting Read Only Access for Analysis

Someone with owner access to the ```equitable-polling-locations``` project can grant read only access to the datasets by:
* Going to the equitable_polling_locations_prod [dataset from the console](https://console.cloud.google.com/bigquery?ws=!1m4!1m3!3m2!1sequitable-polling-locations!2sequitable_polling_locations_prod).
* Under the explorer menu, and click: the vertical elipses -> Share -> Manage Permissions -> ADD PRINCIPAL.
* From the Add Principal menu, invite the person to be added and include the roles "Big Query Data Viewer"

With read only access, the user can

**NOTE:** the principal will need their own or an existing Google Project they have access to in order for BigQuery to bill that project (as opposed to billing the equitable-polling-locations for queries), so a google email account is recommended.  For more details on permissions [watch this video](https://www.youtube.com/watch?v=YfXm3_VsFXY&list=PLFHcsNl_5q_8FGF2nsU6YCXAaCMeQjmsG&index=2).


## Selecting which database to write to

When using the model_run_cli.py or db_import_cli.py scripts as well as Alembic for database migrations, the The Google Project
and BigQuery dataset can be selected by setting the environemntal variables DB_PROJECT and DB_dataset.
If these variables are not set then you will be prompted to chooose which project and which dataset to use.

Setting Project and dataset for Linux/MacOS
```bash
export DB_PROJECT=equitable-polling-locations
export DB_DATASET=equitable_polling_locations_production
```

Setting Project and dataset for Windows
```bash
set DB_PROJECT=equitable-polling-locations
set DB_DATASET=equitable_polling_locations_production
```


## Writting model run output to the database

To write ouput from the model_run_cli to Google's BigQuery instead of local csv files, opt for the option ```-o db```. Database write access will be required.

Example:

```
python ./model_run_cli.py -c1 -o db -vv -l logs Gwinnett_County_GA_original_configs/Gwinnett_config_original_2020.yaml
```


## Import existing csv files into the BigQuery database

To import existing csv files into  the BigQuery database, use the db_import_cli.py script.

Here is an example of importing all results from Berkeley_County_SC_original_configs:

```
python db_import_cli.py ./Berkeley_County_SC_original_configs/*.yaml
```

Any errors importing will be written to the screen as well as the logs directory (by default) to the file `.../logs/import_errors.csv`.

# Analysis and Google Cloud Storage

By default output from R analysis will be written to Google Cloud Storage under the bucket `equitable-polling-analysis` under the folder `result_analysis`. This bucket can be browsed in the [Google Cloud Storage](https://console.cloud.google.com/storage/browser/equitable-polling-analysis;tab=objects?forceOnBucketsSortingFiltering=true&project=equitable-polling-locations&prefix=&forceOnObjectsSortingFiltering=false) console, if access is grated.

## Database

When running analsyis in R, such as the `.../result_analysis/Basic_analsysis.r`, data can be read from the local csv files or from
a BigQuery dataset.  If data is desired from the database, set READ_FROM_CSV to FALSE, such as:

```R
READ_FROM_CSV = FALSE
```

Database authentication will happen automatically via the [`bigrquery` R library](https://bigrquery.r-dbi.org/).

To set which database to use, set the `PROJECT`, `DATASET`, and `BILLING` variables, such as the following:

```R
PROJECT = "equitable-polling-locations"
DATASET = "equitable_polling_locations_prod"
BILLING = PROJECT
```

## Google CloudStorage

To write the output of analysis to the Google Cloud Cloud storage, at the end of the analysis file make sure the following variables are set.

| Variable Name               | Description                                                                                 |
|-----------------------------|---------------------------------------------------------------------------------------------|
| STORAGE_BUCKET              | The google cloud storage bucket to write to.                                                |
| CLOUD_STORAGE_ANALYSIS_NAME | The name of the folder to write all the analysis to under the folder `.../result_analysis/` |


And at the end of the R file, add the following line:

```R
upload_graph_files_to_cloud_storage()
```

Authentication will happen via the [`googleCloudStorageR` R library](https://CRAN.R-project.org/package=googleCloudStorageR).  Before running the R analysis, you must login via the [gcloud command line tool](https://cloud.google.com/sdk/docs/install-sdk).  One installed, run the following:

```
gcloud auth application-default login
```


Example:
```R
# ...

STORAGE_BUCKET = "equitable-polling-analysis"
CLOUD_STORAGE_ANALYSIS_NAME = "Basic_analysis.r"

# ...

###maps####

sapply(orig_list_prepped, function(x)make_bg_maps(x, 'map'))

sapply(orig_list_prepped, function(x)make_demo_dist_map(x, 'population'))
sapply(orig_list_prepped, function(x)make_demo_dist_map(x, 'black'))
sapply(orig_list_prepped, function(x)make_demo_dist_map(x, 'white'))
sapply(orig_list_prepped, function(x)make_demo_dist_map(x, 'hispanic'))
sapply(orig_list_prepped, function(x)make_demo_dist_map(x, 'asian'))

upload_graph_files_to_cloud_storage()
```

By setting the above, any graph file written to using the functions found in `.../result_analysis/graph_functions.R` and `.../result_analysis/graph_functions.R` will be written locally in the `.../result_analysis/` as well as to Cloud Storage under the folder `equitable-polling-analysis:result_analysis/Basic_analysis.r/[DATESTAMP]/...`.


# Google Colab

**NOTE:** Google Colab notebooks are currently out of date as the team has transitioned to running locally.  The following will need to be revisited.

From Google Colab:
* For example, follow the the instructions in [this file](./Colab_runs/colab_Gwinnett_expanded_multi_11_12_13_14_15.ipynb) (To be accessed in the directory of the Equitable-Polling-Locations git repo)

# Input data
There are three sets of data needed to run the optimization and analysis in this program: Census data for the county, aggregated at the block and block group level; a *manually generated* dataset of past and potential polling locations, consistent with local laws; and  a config file that contains the parameters for a given optimization. Additionally, if one wishes to analyze driving distances, there is an additional input file to capture this data.

## Config data

In the database, config data is stored with a unique `config_set` and `config_name` pair. When stored locally, `config_set` corresponds to the config_folder, while `config_name` corresponds to the file (.yaml) in the config_folder. 

There may be multiple `config_name`s sharing the same `config_set`. However, each of these datasets can only differ from each other by a single field (aside from `config_name`, `id` and `created_at` fields). **If this property does not hold, the analysis files will not run.**

### Creating config data
To create config data, create an examplar config file and put it in the desired folder. The exemplar config file should not end in `.yaml`. In these example, the exemplar files ends in `.yaml_template`

Then run

 `python auto_generate_configs.py -b 'config_folder/exemplar_config.yaml_template' -f 'field_to_change' -n list of values field_to_change should take`

This will create a set of .yaml files in the indicaded `config_folder`, each with a different name (that is a combination of the indicated `field_to_change` and a value from the provided list.) It will also write these configs to the database.

**Note:** 
* If a file by the config name already exists in the config_folder, the script will not run
* If the list of values following -n is a list of list, each sublist need to be entered separately.
* The fields of the exemplar_config MUST match the fields in the sql_alchemy model. Otherwise, this script will not run.

**Example:**

To generate a set of configs for DuPage County, IL where the number of precincts open varies from 15 to 20, run
```
python auto_generate_config.py -f 'DuPage_County_IL_potential_configs/example_config.yaml_template' -b 'precincts_open' -n 15 16 17 18 19 20
```
To generate a set of configs for DuPage County, IL where the set of bad locations varies are ['Elec Day School - Potential', 'Elec Day Church - Potential', 'bg_centroid'],  ['Elec Day Church - Potential', 'bg_centroid'], ['Elec Day School - Potential',  'bg_centroid'], and [ 'bg_centroid'] run 
```
python auto_generate_config.py -f 'DuPage_County_IL_potential_configs/example_config.yaml_template' -b 'bad_locations' -n 'Elec Day School - Potential' 'Elec Day Church - Potential', 'bg_centroid' -n  'Elec Day Church - Potential' 'bg_centroid' -n 'Elec Day School - Potential' 'bg_centroid' -n 'bg_centroid'
```
### Config fields
These fields are determined by the sql_alchemy config model. See `models/model_config.py`. In addition to the fields listed below, and `id`, and `created_at` field are generated when uploaded to the database.

* config_set
    * str
    * The name of the set of configs that this config belongs to.
* config_name 
    * str 
    * The name of this model config. '''
* location 
    * str, nullable
    * Location for this model. Usually a county.
* year 
    * List[str], nullable 
    * An array of years of historical data relevant to this model
* bad_types
    * List[str], nullable
    * A list of location types not to be considered in this model
* beta
    * float, nullable  
    * level of inequality aversion: [-2,0], where 0 indicates indifference, and thus uses the mean. -1 is a good number.
* time_limit
    * float, nullable
    * How long the solver should try to find a solution
*  penalized_sites
    * List[str], nullable 
    * A list of locations for which the preference is to only place a polling location there if absolutely necessary for coverage.  A site in this list should be selected only if it improves access by x meters, where x is calculated according to the problem data. (See https://doi.org/10.48550/arXiv.2401.15452 for more information.) This option generates three additional log files: two for additional calls to the optimization solver ("...model2.log", "...model3.log") third ("...penalty.log") providing statistics related to the penalty heuristic.
* precincts_open
    * int, nullable
    * The total number of precincts to be used this year. 
    * If no user input is given, this is calculated to be the number of
    polling places identified in the data.
* maxpctnew
    * float, nullable
    * The percent on new polling places (not already defined as a
    polling location) permitted in the data. 
    * Default = 1. I.e. can replace all existing locations
* minpctold
    * float, nullable
    * The minimun number of polling places (those already defined as a
    polling location) permitted in the data. 
    * Default = 0. I.e. can replace all existing locations
* max_min_mult
    * float, nullable
    * A multiplicative factor for the min_max distance caluclated
    from the data. Should be >= 1. 
    * Default = 1.
* capacity
    * float, nullable
    * A multiplicative factor for calculating the capacity constraint. Should be >= 1.
    * Default = 1. 
    * Note, if this is not paired with fixed_capacity_site_number, then the capacity changes as a function of number of precincts.
* fixed_capacity_site_number
    * int, nullable
    * The default number of open precincts if one wants to hold the number of people that can go to a location constant (as opposed to a function of the number of locations).
* driving
    * bool, nullable
    * Driving distances used if True and distance file exists in correct location
* log_distance
    * bool, nullable
    * Flag indicating whether or not the log of the distances is to be used in the optimization

## **Census Data (demographics and shapefiles)**:
The sofware requires a free census API key to run new counties. You can [apply on the cenus site](https://api.census.gov/data/key_signup.html) and be approved in seconds.

    1. Create the directory authentication_files/
    2. Inside authentication_files/ create a file called census_key.py
    3. The file should have a single line reading: census_key = "YOUR_KEY_VALUE"

If you are only running counties already in the repo you skip this step. However, it is needed to run counties for which data does not exist locally.

The script `pull_census_data.py`, which can also be run from the command line, pulls the following files from the 2020 US Census:
1. P3 (race) and P4 (ethnicity) files for the indicated county, at both the block and the block group level
    1. This is saved locally in the folder `datasets/census/redistricting`
2. Tiger shape files for the county at both the block and block group level.
    1. This is saved locally in the folder `datasets/census/tiger`

Eventually, this data will be loaded to the database as well. Until then, it is either stored locally, or needs to be downloaded for each call to `model_run_cli.py`.

<!--

If you are interested in only running results for  Gwinnett County, no further action is needed. If you are interested in running a county for which you do not have the above data, the software will notify you that the necessary data is missing.

instructions for downloading or creating these files and their formats are given here.

All file paths are given relative to the git folder for Equitable-Polling-Locations

### **datasets/census/redistricting/County_ST/DECENNIALPL2020.P3-Data.csv**:
* This is the census dataset for a racial breakdown of people of voting age by census block.
* Documentation for this dataset can be found [on the census api site for P3](https://api.census.gov/data/2010/dec/sf1/groups/P3.html)
* Instructions for downloading this data:
    * Visit [RACE FOR THE POPULATION 18 YEARS AND OVER](https://data.census.gov/table?q=P3:+RACE+FOR+THE+POPULATION+18+YEARS+AND+OVER&tid=DECENNIALPL2020.P3)
    * Select Geography:
    * Filter for Geography -> Blocks -> State -> County Name, State -> All Blocks within County Name, State
    * If asked to select table vintage, select 2020;  DEC Redistricting Data (PL-94-171)
    * Unzip and place the contents of the downloaded folder in 'datasets/census/redistricting/Count_ST/'
* Columns of P3 selected by the software:
    * White alone
    * Black or African American alone
    * American Indian And Alaska Native alone
    * Asian alone
    * Native Hawaiian and Other Pacific Islander alone
    * Some Other Race alone
    * Two or More Races

### **datasets/census/redistricting/County_ST/DECENNIALPL2020.P4-Data.csv**:
* This is the census dataset for a racial breakdown of people of voting age by census block.
* Documentation for this dataset can be found [on the census api site for P4](https://api.census.gov/data/2010/dec/sf1/groups/P4.html)
* Instructions for downloading this data:
    * Visit [HISPANIC OR LATINO, AND NOT HISPANIC OR LATINO BY RACE FOR THE POPULATION 18 YEARS AND OVER](https://data.census.gov/table?g=050XX00US13135$1000000&d=DEC+Redistricting+Data+(PL+94-171)&tid=DECENNIALPL2020.P4)
    * Select Geography:
    * Filter for Geography -> Blocks -> State -> County Name, State -> All Blocks within County Name, State
    * If asked to select table vintage, select 2020;  DEC Redistricting Data (PL-94-171)
    * Unzip and place the contents of the downloaded folder in 'datasets/census/redistricting/County_ST/'
* Columns of P4 selected by the software:
    * Total population
    * Total hispanic
    * Total non_hispanic
### **datasets/census/tiger/County_ST/tl_YYYY_FIPS_tabblockYY.shp**:
[TIGER/line Shapefiles](https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html) is a database of shape files for the geographic categories used by the census.
* Documentation: https://www.census.gov/programs-surveys/geography/technical-documentation/complete-technical-documentation/tiger-geo-line/2020.html
* Instructions for downloading this data:
    * Visit https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.2020.html#list-tab-790442341
    * Scroll down to FTP Archive by State
    * Click on desired States
    * Click on desired FIPS Code for the County
    * Download tl_YYYY_FIPS_tabblockYY.zip (e.g. tl_2020_13135_tabblock20.zip)
    * Unzip and place the contents of the downloaded folder in 'datasets/census/tiger/County_ST/'
* Columns of block geography selected by the software:
    * GEOID20 - identifier. Format:1000000USFIPSCODEBLOCKNUM, e.g. 1000000US131510703153004
    * geometry - the polygon of the block
    * INTPTLAT20 - latitude of block centroid
    * INTPTLON20 - longitude of block centroid

### **datasets/census/tiger/County_ST/tl_YYYY_FIPS_bgYY.shp**:
The instructions for downloading this data is identical the instructions for the blocks with the following exception:
* Download tl_YYYY_FIPS_bgYY.zip (e.g. tl_2020_13135_bg20.zip)

-->

## Manually constructed data for historical and admissible polling locations

This is a manually constructed .csv file that contains data for existing and potential polling locations to be optimized against. In the current state, this is not on the database. Instead, it should be created locally at  `datasets/polling/County_ST/County_ST_locations_only.csv`

Example file name: datasets/polling/Gwinnett_County_GA/Gwinnett_County_GA_locations_only.csv


The columns of this data set should be named and formatted as
|Column Name | Definition | Example |
| ----- | ------ | ----- |
|Location | Name of the actual or potential polling location | 'Bethesda Senior Center' |
| Address | Street Address of the actual or potential polling location| (format flexible) '788 Hillcrest Rd NW, Lilburn, GA 20047' |
|Location Type | If polling location, must have a year when it was used | 'EV_2022_2020' or 'General_2020' or 'Primary_2022_2020_2018' or 'DropBox_2022'|
| | If potential location, has a 'location type' category and the word 'Potential' (case sensitive) | 'Community Center - Potential' |
| Lat, Lon | Comma separated concatenation of latitude and longitude (can be read off of google maps by right clicking on the location marker for the address.) | '33.964717796407434, -83.85827288222517' |

## OPTIONAL: Driving distances

If you are using driving distances (that have been calculated externally) in the optimization, place a file at `datasets/driving/County_ST/County_ST_driving_distances.csv`.  This file will only be accessed if the optional parameter 'driving' is set to True. 

Example file name: datasets/driving/Gwinnett_County_GA/Gwinnett_County_GA_driving_distances.csv
The columns are as follows:
|Column Name | Definition | Example |
| ----- | ----- | ----- |
| id_orig | Census block id that matches the 'FIPSCODEBLOCKNUM' portion of the GEOID column from the file datasets/census/tiger/County_ST/tl_YYYY_FIPS_tabblockYY.shp file | 131510703153004 |
| id_dest | Name of potential polling location, as in the Location column of the file datasets/polling/County_ST/County_ST_locations_only.csv. | 'EV_2022_2020' or 'General_2020' or 'Primary_2022_2020_2018' or 'DropBox_2022' |
| distance_m | Driving distance from id_orig to id_dest in meters | 10040.72 |


# Logging

## Working with code run from the command line interface

Currently the logging system in this project is a bit overly simplistic - they are print statements that are only run if the boolean variable "log" passed around is set to ```True```.  The logging used in the project is intended to work from the command line as well as from instances of Jupyter notebooks.  Processes may be run concurrently so simply writing to the screen or a single log file will not work since one process may print to the screen at the same time as another. As such, all screen prints are suppressed unless multiple concurrency is disabled AND verbose mode is specified (```-c0 -v``` on the command line).

When running from the command line, model_run.py will be called from model_run_cli.py.  model_run_cli.py will parse all the command line arguments and call the function ```run_on_config``` found in model_run.py using multiple concurrent processes as requested by the user based on the concurrency option ```-c```.  Each call concurrent to run_on_config will contain individual instances of ```PollingModelConfig``` from model_config.py which is a simple container class to pass all the configuration needed to run pyomo/SCIP.

When multiple concurrency is selected, as discussed further in the "To run" section of this document, logs will be written to the log directory specified by the user when run from the command line interface instead of the screen, typically the directory ```./logs``` instead of the screen.

PollingModelConfig will be setup with all the information needed to run a model, including where to write logs to in the variable ```log_file_path```, which is a string to the specific file that should be written (appended) to. The value of  ```log_file_path``` from PollingModelConfig is what is passed to pyomo so that it will write its log output to the correct location.   The individual log files will be named after the config file being run prefixed with a time stamp.  e.g.
```./logs/20231207151550_Gwinnett_config_original_2020.yaml.log```.


### Using logs to debug

Until the logging system is updated to something more robust, any additional logging needed should be done with print statements that respect the ```logging``` boolean variable for use when concurrency is set to single threaded . Alternatively the file path ```log_file_path``` specified in the PollingModelConfig instance can be appended to.

If output from these log statements are needed then it is suggested that the command line be run in single concurrency mode with verbosity set to maximum e.g.:

```
python ./model_run_cli.py -c1 -vv -l logs ./Gwinnett_County_GA_configs/Gwinnett_config_expanded_*.yaml
```

When running concurrently, logs can be followed from the log directory in realtime using something like the following in Linux/MacOS:

```
tail -f ./logs/20231207151550_Gwinnett_config_original_2020.yaml.log
```

# Intermediate dataset
Currently, the optimizer checks for the existence of a specific dataset before running. A few notes:
* This dataset is currently created and stored locally. 
    * `datasets/polling/County_ST/County_ST.csv`
    * This will change in the future
* Currently, this dataset creates a column with the haversine distance (which is the default distance for the optimizer). 
    * In future, this functionality will change,
    * Likely will allow for other columns to be created and stored, rather than run real time.

### County_ST.csv
This is the main data set that the optimizer uses. It includes polling locations from previous years, potential polling locations, and block group centroids, as well as distances from block centroids to the above.

Example file name: datasets/polling/Gwinett_County_GA/Gwinnett_County_GA.csv

The columns of this data set are as follows:
|Column Name | Definition | Derivation | Example / Type |
| ----- | ------ | ----- | ----- |
|Fields for matching destinations and origins|
|  |
|id_orig | Census block code | GEOID20 from block shape file | 131350501051000 |
|id_dest | Name of the actual or potential polling location | 'Location' from County_ST_location_only.csv | 'Bethesda Senior Center' |
| county | name of county and two letter state abbreviation | location from the config file | 'Gwinnett_County_GA' |
| address | If a physical polling location, street address | 'Address' from County_ST_location_only.csv  | '788 Hillcrest Rd NW, Lilburn, GA 20047'|
| | If not a physical location, name of the associated census block group | | STRING |
| dest_lat | latitude of the address or census block group centroid of the destination | google maps or INTPTLAT20 of id_dest from block group shape file| FLOAT |
| dest_lon | longitude of the address or census block group centroid of the destination | google maps or INTPTLON20 of id_dest from block group shape file| FLOAT |
| orig_lat | latitude of census block centroid of the origin | INTPTLAT20 of id_orig from block shape file| FLOAT |
| orig_lon | longitude of census block centroid of the origin | INTPTLON20 of id_orig from block shape file| FLOAT |
|location_type | A description of the id_dest location | 'Location Type' from County_ST_location_only.csv or 'bg_centroid' | 'EV_2022_2020' or 'Library - Potential' or 'bg_centroid'|
| dest_type | A coarser description of the id_dest that given in location type | Either 'polling' (if previous polling location), potential (if a building that is a potential polling location), 'bg_centroid' (if a census block centroid) |
|Distance fields|
|| 
|distance_m | distance in meters from the centroid of the block (id_orig) to id_dest | distance from (orig_lat, orig_lon) to (dest_lat, dest_lon) | FLOAT |
|source| type of distance, currently supported: (log) haversine, (log) driving | from log_distance and driving flag in config file |STRING |
|Demographic fields|
|| 
| population | total population of census block | 'P3_001N' of P3 data or 'P4_001N' of P4 data| INT |
| hispanic | total hispanic population of census block | 'P4_002N' of P4 data| INT |
| non_hispanic | total non-hispanic population of census block | 'P4_003N' of P4 data| INT |
| white | single race white population of census block | 'P3_003N' of P3 data | INT |
| black | single race black population of census block | 'P3_004N' of P3 data | INT |
| native | single race native population of census block | 'P3_005N' of P3 data | INT |
| asian | single race asian population of census block | 'P3_006N' of P3 data | INT |
| pacific_islander | single race pacific_islander population of census block | 'P3_007N' of P3 data | INT |
| multiple_races | total multi-racial population of census block | 'P3_009N' of P3 data | INT |
| other | single race other population of census block | 'P3_008N' of P3 data | INT |

# Output datasets

For each `config_set` and `config_name` pair, the optimizer produces 4 output files.
* The output files have the names:
    * County_config_DESCRIPTOR_edes.csv
    * County_config_DESCRIPTOR_precinct_distances.csv
    * County_config_DESCRIPTOR_residence_distances.csv
    * County_config_DESCRIPTOR_result.csv

* If the file was run with the `-o 'csv'` flag, the outputs are written in the folder County_ST_results/ with the following names:
    * CONFIG_FOLDER.County_config_DESCRIPTOR_edes.csv
    * CONFIG_FOLDER.County_config_DESCRIPTOR_precinct_distances.csv
    * CONFIG_FOLDER.County_config_DESCRIPTOR_residence_distances.csv
    * CONFIG_FOLDER.County_config_DESCRIPTOR_result.csv
* Otherwise, the most recent data can be pulled from the `*_extras` table, where * is one of `edes`, `precinct_distances`, `residence_distanes` or `result`.

* The four files can be described as follow:
    * *_edes (demographic level ede scores)
        * For each demographic group (asian, black, hispanic, native, population, white), this table records the
            * demo_pop, the total population of that demographic in the county
            * average distance traveled by the members of that demographic: average_distance = weighted_distance / demo_pop
            * the y_EDE for the demographic: y_EDE = -1/(beta * alpha)*log(avg_KP_weight)
                * where avg_KP_weight= (\sum demo_res_obj_summand)/demo_pop
    * *_precinct_distances.csv (distances traveled to each precinct by demographic)
        * For each demographic group (asian, black, hispanic, native, population, white), and identified polling location (id_dest), this table records the
            * demo_pop, the total population of that demographic matched to that location
            * average distance traveled by the members of that demographic: average_distance = weighted_distance / demo_pop
    * *_demographic_distances.csv (distances traveled by members of a census block to each polling location by demographic)
        * This is an interim table needed to create the *_ede.csv table
        * For each demographic group (asian, black, hispanic, native, population, white), and census block (id_orig), this table records the
            * demo_pop, the total population of that demographic matched to that location
            * average distance traveled by the members of that demographic: average_distance = weighted_distance / demo_pop
    * *_result.csv (a combined table of census block, matched polling location, distance, and demographic information)
        * This is a source table for the above three
        * For each census block (id_orig), this table records the
            * polling location (id_dest) to which the census block is matched
            * the distance to this polling location
            * the source of this distance measure
            * The County_ST of the run
            * the address of the the polling location (if it exists)
            * the coordinates of the block centroid (orig_lat and orig_lon) and the coordinates of the destination (dest_lat and dest_lon)
            * population of each of the demographic groups per census block
            * It also reports weighted distance and KP factor, which are population level variables, but these columns are never used and should be removed in a future release.

# result_analysis
After the data is generated by the optimizer, it is analysed by function in the `result_analysis/` folder. Generated graphs and maps for a config_folder (or config_set) are placed in a subfolder of `result_analysis/` folder as well as the Google CloudStorage bucket indicated in the file.

## Overall workflow

Whether pulling from the database or csv, the analysis software compiles the relevant data as follows:
1. Pull all the data from the indicated config_sets
    1. The graphs need the all the data for a config_set together
    1. The maps need each config_name, config_set pair separately
1. Performs safety checks
    1. That the data pulled varies by only one field per set
    1. That the data exists
    1. That the data contains the locations of interest
1. Pulls out a Driving Flag
    1. Will likely want a Log distance flag as well soon
1. Reads and coallates data by the four tables
    1. This includes and autmatic generation of a descriptor field based off the field that varies
    1. This descriptor field may need to be mannually changed, depending on the specifics of the data.
1. For mapping, this is then combined with block group level demographic data, block group level distances are calculated and each config_name, config_set pair is split into its own datatable
1. Graphs are made and stored
    1. edes and average distances by population are plotted together for an indicated set of config_names
    1. edes by demographic are plogged for a given config_set
    1. a plot of which precincts are chosen, for which optimization, how many people are assigned for all config_names in a config_set
    1. A histogram comparing the distributions of how far people travel for an indicated set of config_names
1. Maps are made and stored
    1. A heat map of how far people have to travel with polling locations indicated
    1. A head map of how far people of a given demographic have to travel. 


# Acknowledgements

Our tool uses the SCIP mixed-integer optimization solver:

[SCIP Optimization Suite 8.0](https://www.scipopt.org/index.php#about) <br />
*Ksenia Bestuzheva, Mathieu Besançon, Wei-Kun Chen, Antonia Chmiela, Tim Donkiewicz, Jasper van Doornmalen, Leon Eifler, Oliver Gaul, Gerald Gamrath, Ambros Gleixner, Leona Gottwald, Christoph Graczyk, Katrin Halbig, Alexander Hoen, Christopher Hojny, Rolf van der Hulst, Thorsten Koch, Marco Lübbecke, Stephen J. Maher, Frederic Matter, Erik Mühmer, Benjamin Müller, Marc E. Pfetsch, Daniel Rehfeldt, Steffan Schlein, Franziska Schlösser, Felipe Serrano, Yuji Shinano, Boro Sofranac, Mark Turner, Stefan Vigerske, Fabian Wegscheider, Philipp Wellner, Dieter Weninger, Jakob Witzig* <br />
Available at [Optimization Online](http://www.optimization-online.org/DB_HTML/2020/03/7705.html) and as [ZIB-Report 21-41, December 2021](http://nbn-resolving.de/urn:nbn:de:0297-zib-78023)


# How to cite

If you use the Equitable-Polling-Locations code base in your work, please cite the following:

1. The github repository using the "Cite this repository" dropdown menu

2. Horton, D., Logan, T., Murrell, J., Speakman, E. & Skipper, D. (2024). *A Scalable Approach to Equitable Facility Location.*  https://doi.org/10.48550/arXiv.2401.15452

